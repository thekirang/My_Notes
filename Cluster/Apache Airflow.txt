APACHE AirFlow:

Apache Airflow is a workflow scheduler that automates and manages different types of jobs, including Spark jobs.  
It schedules workflows using DAGs (Directed Acyclic Graphs), which define the order of task execution.

Directory Structure Example:
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ my_first_dag.py
â”‚   â””â”€â”€ etl/
â”‚       â””â”€â”€ etl_dag.py
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ my_custom_plugin.py
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ my_first_dag/
â”œâ”€â”€ airflow.cfg


Key Concepts:

- Task: A single job or step in the workflow (e.g., running a script or moving a file).

- DAG (Directed Acyclic Graph): A structure that defines the order in which tasks should run.  
  Ensures tasks run in the correct sequence and only once.

- Operator: Determines how a task runs.  

ðŸ”¹ If a task is defined in a Python function, then Airflow uses PythonOperator.

ðŸ”¹ If a task is defined using Linux shell commands, then Airflow uses BashOperator.

ðŸ”¹ If a task involves sending an email, then Airflow uses EmailOperator.

ðŸ”¹ If a task involves transferring files to/from SFTP, then Airflow uses SFTPOperator.

ðŸ”¹ If a task involves running SQL queries on a database, then Airflow uses SQLExecuteQueryOperator or a DB-specific one like:
    â€¢ PostgresOperator (for PostgreSQL)
    â€¢ MySqlOperator (for MySQL)
    â€¢ SnowflakeOperator, etc.

ðŸ”¹ If a task involves scheduling a Spark job, then Airflow uses SparkSubmitOperator.

ðŸ”¹ If a task involves triggering another DAG, then Airflow uses TriggerDagRunOperator.

ðŸ”¹ If a task involves executing a Docker container, then Airflow uses DockerOperator.

ðŸ”¹ If a task involves executing a Kubernetes pod, then Airflow uses KubernetesPodOperator.

ðŸ”¹ If a task involves interacting with Google Cloud, then Airflow uses GCP operators like:
    â€¢ GCSToBigQueryOperator
    â€¢ BigQueryOperator
    â€¢ GCSDeleteObjectsOperator

ðŸ”¹ If a task involves interacting with AWS, then Airflow uses AWS operators like:
    â€¢ S3CreateObjectOperator
    â€¢ LambdaInvokeFunctionOperator
    â€¢ GlueJobOperator


Main Components of Airflow:

- Scheduler: Decides when tasks should run based on the DAG.

- Executor: Runs the tasks (either sequentially or in parallel).

- Webserver: Provides a web UI to monitor workflows and check their status.

- DAG Files: Python files where tasks, scheduling, and dependencies are defined.

- Metadata Database: Stores workflow history, task status, and configuration.


Quick Summary Table:

Term              | What it Means
------------------|--------------------------------------------------
DAG               | Overall workflow and task order
Task              | A single job in the workflow
Operator          | Defines how a task is run (Python, Bash, etc.)
Scheduler         | Decides when tasks should run
Executor          | Executes the tasks
Webserver         | UI to manage and monitor workflows
Metadata Database | Stores task history and workflow information


How Airflow and Spark Work Together:

- Airflow DAGs are Python scripts that define the structure and timing of a workflow.
- Spark jobs are separate scripts or JAR files (written in PySpark, Scala, or Java) that contain the actual data processing logic.

Running Spark Jobs from Airflow:

- Use operators like SparkSubmitOperator or SparkSqlOperator within your DAG to submit Spark jobs.
- These operators point to your external Spark code and send them to the Spark cluster for execution.
- Airflow manages when and how the Spark jobs run, but the actual logic stays in the Spark scripts.

