1. Data Collection  
   - Circle-wise collectors gather two types of raw files from each site:  
     • CGF (user metadata)  
     • CGNAT (usage records)  
   - At some sites, we receive pre-stitched EDR files (combined CGF+CGNAT). At others, we must stitch CGF and CGNAT together ourselves.

2. Landing & Validation  
   - All incoming files land in our Landing Servers at respective paths 
   - I validate each batch’s format (timestamps, IP addresses—private, public, translated—and session durations) as part of site go-lives.  
   - A reconciliation pass confirms receipt of the expected ~100 files per batch from SNOC/collector nodes.

3. Ingestion  
   - Once validated, raw files are ingested into our Hadoop cluster (99 nodes, ~10 PB storage).

4. Pre-processing & EDR Stitching  
   - For sites without pre-stitched EDR, we run Spark jobs to correlate CGF and CGNAT records into unified EDR datasets.  
   - These same Spark jobs extract data from HDFS as needed by downstream teams (CGF, CGNAT, EDR, nodal analytics).

5. ETL with Apache Airflow  
   - Apache Airflow orchestrates Spark workflows:  
     • Extract raw or stitched data from HDFS  
     • Transform and enrich it  
     • Load the results into Hive tables distributed across the cluster  

6. Scheduling & Maintenance  
   - Lightweight tasks and housekeeping on Linux servers run via cron jobs.  
   - Airflow’s DAG UI and Spark’s web UI give visibility into job progress and failures.

7. Monitoring & Alerting  
   - Ambari Portal provides cluster-wide health metrics and alerts (node status, disk usage, YARN health).  
   - Grafana dashboards track real-time data ingestion rates, write throughput, and extraction activity.

8. Day-to-Day Operations  
   - I continuously watch Ambari and Grafana, investigate any performance bottlenecks or failed jobs, and raise tickets or fix issues to keep the ETL pipeline running smoothly.  
   - Our goal is to ensure that customers receive accurate, timely insights every day from the hundreds of gigabytes we process.
