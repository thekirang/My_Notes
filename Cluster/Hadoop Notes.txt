Hadoop: 
Hadoop is an open-source software framework for distributed storage and processing of large datasets, commonly referred to as big data

ðŸ§± Core Hadoop Components:

1. HDFS (Hadoop Distributed File System)
2. YARN (Yet Another Resource Negotiator)
3. MapReduce (replaced by Apache Spark)

Explaination:

1. HDFS (Hadoop Distributed File System)
ðŸ“‚ Storage layer of Hadoop

Stores large files by splitting them into blocks (default 128MB).

Saves each block across multiple DataNodes (replication).

Allows fault tolerance and scalable storage.

Key Subcomponents:
NameNode â€“ The master; keeps metadata (file names, block locations).

DataNode â€“ The workers; actually store the blocks of data.

2. YARN (Yet Another Resource Negotiator)
ðŸ§  Resource management layer

Manages CPU + memory resources across the cluster.

Allocates containers for tasks.

Separates resource management from data processing.

Key Subcomponents:
ResourceManager (RM) â€“ Global resource manager.

NodeManager (NM) â€“ Runs on each worker node to manage local resources.

3. MapReduce
ðŸ§® Batch processing engine (older)

Programming model for processing large data sets in parallel.

Two phases:

Map: Break input into key-value pairs.

Reduce: Aggregate or summarize data.

ðŸ”¹ (Note: In modern setups, MapReduce is often replaced by Apache Spark for faster processing.)

4. Common / Core Utilities
ðŸ§° Shared Java libraries and utilities

Required by all Hadoop components.

Includes I/O libraries, configuration, RPC, serialization, etc.

-------------------------------------------------------------------------------

HADOOP COMPONENTS OVERVIEW
==========================

NAMENODE
--------
- Keeps track of which file is stored on which DataNodes (does not store actual data).
- Stores metadata such as file permissions, directory structure, and replication information.
- When a user wants to read/write a file, the NameNode provides the location of the data.
- If the NameNode fails, the entire HDFS becomes unavailable.

DATANODE (STORES ACTUAL DATA)
-----------------------------
- A worker node that stores actual data blocks.
- Works under the control of the NameNode.
- Sends regular heartbeats to the NameNode to indicate it's alive.
- If a DataNode fails, the NameNode initiates replication from other DataNodes.

SIMPLE ANALOGY: HDFS AS A LIBRARY
---------------------------------
- NameNode = Librarian (keeps catalog/index but doesnâ€™t hold books).
- DataNodes = Bookshelves (store actual books/files).

SECONDARY NAMENODE/check pointing (only comes into picture when HA is not enabled)
------------------
- Not a backup for the NameNode!
- Periodically saves metadata snapshots and merges edit logs with the file system image.
- Helps reduce load on the NameNode and aids in faster recovery.
- Does not take over if the NameNode crashes.
- Analogy: Like a backup assistant librarian who organizes catalog updates for reference.

Standby NameNODE (when HA is enabled)
-----------
- An active backup of the NameNode.
- Maintains a real-time copy of the NameNodeâ€™s metadata.
- Can take over immediately if the NameNode fails.
- Analogy: A co-librarian with a live mirror of the records.

EDGE NODE
---------
- Acts as a gateway for external applications to interact with the Hadoop ecosystem.
- Typically where client tools and scripts run.

--------------------------------------------------

Block Structure
---------
-Enables distributed storage and replication for scalability and fault tolerance

YARN WORKFLOW (SIMPLIFIED)
==========================

1) Submit your application to the ResourceManager.
2) ResourceManager allocates a dedicated ApplicationMaster for your job.
3) ApplicationMaster analyzes the job and requests required resources (CPU, memory).
4) ResourceManager assigns containers after considering resource availability.
5) ApplicationMaster coordinates with NodeManagers to start containers.
6) Your application runs inside these containers, processing data.
7) ApplicationMaster monitors execution, handles failures, and may request more resources.
8) Once completed, ApplicationMaster notifies the ResourceManager and releases containers.

--------------------------------------------------

THE BIG PICTURE: HOW IT ALL FITS TOGETHER
=========================================

- DATA STORAGE LAYER:
    * HDFS stores large-scale datasets across many machines.

- RESOURCE MANAGEMENT LAYER:
    * YARN allows multiple data processing applications to share cluster resources efficiently.

- PROCESSING FRAMEWORKS:
    * Spark, MapReduce, and others perform distributed computation.

- DATA ACCESS LAYER:
    * Hive, HBase, and Pig allow users to interact with the data in different ways.

- COORDINATION SERVICES:
    * ZooKeeper ensures distributed services operate in sync and handle failure gracefully.

- DATA MOVEMENT TOOLS:
    * Flume, Kafka, and Sqoop help ingest/export data to/from the Hadoop ecosystem.

- WORKFLOW MANAGEMENT:
    * Oozie automates and manages complex data processing pipelines.
----------------------------------------------------------------------
- Ambariâ„¢: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. 
  Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.

- Avroâ„¢: A data serialization system.

- Cassandraâ„¢: A scalable multi-master database with no single points of failure.

- Chukwaâ„¢: A data collection system for managing large distributed systems.

- HBaseâ„¢: A scalable, distributed database that supports structured data storage for large tables.

- Hiveâ„¢: A data warehouse infrastructure that provides data summarization and ad hoc querying.

- Mahoutâ„¢: A Scalable machine learning and data mining library.

- Ozoneâ„¢: A scalable, redundant, and distributed object store for Hadoop.

- Pigâ„¢: Pig is used for scripting data flows in Hadoop.

- Sparkâ„¢: A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.

- Submarine: A unified AI platform which allows engineers and data scientists to run Machine Learning and Deep Learning workload in distributed cluster.

- Tezâ„¢: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. 
Tez is being adopted by Hiveâ„¢, Pigâ„¢ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoopâ„¢ MapReduce as the underlying execution engine.

- ZooKeeperâ„¢: A high-performance coordination service for distributed applications.

FINAL THOUGHTS
--------------
- Hadoopâ€™s power lies in its ecosystem, not just one component.
- Each tool solves a specific problem in big data management and processing.
- Mastering Hadoop means understanding which tools to use for which tasks.
