"In my current role, we have pipeline where we are dealing with xyz gb data daily  that we are further injecsting , transforming and storing it.
Also we are fetching meaning insights from this data and serving it to our customer.
in Hadoop cluster consisting of 99 nodes, with a total storage capacity of around 10 petabytes. for managing & monitoring this cluster infrastructure we are using Ambari.
We are using airflow orchestration tool for scheduling our spark jobs for processing and transforming our data that will be storing it in hive Datawarehouse tables in across Hadoop cluster nodes. 
Also We are using spark jobs for processing our data that includes (storing in and fetching out) data from Hadoop cluster.

Day to day responsibilities:
Monitor health and performance of cluster using spark UI and Ambari.
Keeping and eye on processing of our data and maintaining pipeline to ensure data flow smoothly. We are using Grafana from that.
Monitoring dags and inpecting and report issues.
We are using cron jobs for scheduling our task

To effectively monitor the health and performance of the cluster, we’ve integrated Grafana as our primary monitoring tool. 
In Grafana, we keep an eye on data loading, writing, and extraction activities to ensure everything is functioning smoothly.



I’ve been actively involved during go-live activities at multiple sites,
 where I was responsible for validating incoming data formats — 
 including timestamps, IP addresses (private, public, and translated destinations), 
 and session durations.

We’ve also structured our data pipeline using clearly defined landing and reconciliation paths. 
The reconciliation path, for instance, is used to validate the receipt of files — typically around 100 files — sent by the SNOC or collector nodes.

I handle collector-wise data for sources like CGF, CGNAT, and EDR, which we store in Hadoop’s Hive tables.
 For further processing, especially for the nodal teams, we utilize Apache Spark jobs to transform and prepare the data as per their requirements.

These Spark jobs are orchestrated and scheduled using Apache Airflow, which helps automate workflows and 
ensures timely execution and monitoring of all jobs."


daily incoming data ??

ETL = extract transform load