## Introduction

Good morning everyone,

It’s my pleasure to be here for the interview at Infosys.

My name is Kiran Gaikwad. I hold a Bachelor’s degree in Engineering and have 3.4 years of experience spanning Desktop Support, Linux System Administration, and currently, System Analysis.

At present, I am associated with Vodafone Idea, working on the Intellza MS project through Accolite Digital India Pvt. Ltd.

Thank you for giving me this opportunity to introduce myself.

## Project Overview and Workflow

### Project Summary

I am part of a team responsible for managing a high-volume telecom data pipeline. We process and analyze data from multiple telecom circles, handling three main data types:

- **CGF (Charging Gateway Function):** User metadata and charging session records.
- **CGNAT (Carrier-Grade NAT):** IP-level usage and NAT logs.
- **IPDR (Internet Protocol Detail Record):** Stitched records combining CGF and CGNAT data.

These files are sourced from various OEMs such as Nokia, Ericsson, Cisco, Mavenir, and ZTE.

### Data Collection and Validation

- **Data Reception:** Data is received continuously on our landing servers, organized by predefined paths.
- **Validation:** Each incoming file is validated for correct prefix, timestamp, and session IDs. We monitor flow trends with a 5% tolerance compared to D-1 statistics.
- **Volume:** We handle approximately 3.2 million files daily, totaling about 4.8 TB per day. Data is stored on Hadoop HDFS with built-in replication for redundancy (not RAID 3).
- **Error Handling:** Zero-volume or corrupt files are automatically moved to an error path, and we coordinate with upstream teams for resolution.
- **Reconciliation:** We use custom scripts to track file counts, total data volume, and per-circle statistics, ensuring completeness and accuracy.

### Data Processing and Storage

- **Stitching:** For sites where CGF and CGNAT are not pre-stitched, we use Apache Spark to correlate and stitch them into IPDR records.
- **Transformation and Storage:** Processed data is formatted and loaded into Hive tables on a 99-node, 10 PB Hadoop cluster, providing efficient access for nodal teams.
- **Automation:** Apache Airflow orchestrates Spark jobs, automating the entire ETL workflow—from data stitching and transformation to loading and downstream querying.

### Monitoring and Operations

- **Cluster Management:** Ambari Portal is used for provisioning, managing, and monitoring the Hadoop cluster.
- **Performance Monitoring:** Grafana dashboards provide real-time visibility into data ingestion, writing, and extraction rates, helping us proactively address issues.
- **Spark and Airflow UIs:** We use Spark’s web UI for job debugging and Airflow’s UI for tracking task statuses, logs, and schedules.
- **Housekeeping:** Cron jobs handle lightweight maintenance tasks, such as log rotation and temporary file management, to prevent disk usage issues.

### Day-to-Day Responsibilities

- Proactively monitor cluster health and job performance using Ambari, Grafana, and Spark UI.
- Validate and reconcile incoming data batches, ensuring data quality and completeness.
- Investigate and resolve data or pipeline issues in coordination with upstream and downstream teams.
- Maintain and optimize ETL workflows for reliability and efficiency.
- Prepare and enhance dashboards for operational visibility.

## Key Achievements

- **Real-Time Metrics:** Led the implementation of Grafana dashboards for real-time server and data pipeline monitoring, reducing errors and backlogs.
- **Disk Usage Automation:** Automated log rotation and housekeeping to keep disk usage under control, eliminating manual interventions and reducing incidents.

## Motivation for Transition

While I have gained extensive experience in big data pipeline management and automation, I am eager to transition into a cloud-focused role. I want to leverage my skills in automation, infrastructure management, and data engineering on cloud-native platforms like AWS to further my career growth.

## Continuous Learning

I have consistently upskilled myself, moving from desktop support to Linux administration and now to system analysis, taking on increasingly challenging technical responsibilities.

## Suggestions Incorporated

- **Added technical detail:** Provided a clear, step-by-step overview of the data pipeline.
- **Clarified responsibilities:** Outlined day-to-day tasks and specific contributions.
- **Quantified impact:** Included daily data volume, file count, and cluster scale.
- **Corrected technical points:** Replaced RAID 3 with HDFS replication for redundancy.
- **Improved clarity:** Used concise, professional language and consistent terminology.
- **Highlighted automation:** Explained the orchestration role of Airflow and the use of monitoring tools.
- **Specified data types:** Clearly described handling of CGF, CGNAT, and IPDR data.

Thank you for your attention. I look forward to discussing how my experience can contribute to your team at Infosys.